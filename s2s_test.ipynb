{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import librosa\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from natsort import os_sorted\n",
    "import time\n",
    "import copy\n",
    "from torchvision import models\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as transforms\n",
    "from librosa.util import normalize\n",
    "from pypinyin import lazy_pinyin\n",
    "import re\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, root_dir,max_seq_length,stn_length):\n",
    "      self.root_dir = root_dir\n",
    "      self.data_paths = []\n",
    "      self.label = []\n",
    "      self.load_data()\n",
    "      self.max_seq_length = max_seq_length\n",
    "      self.stn_length = stn_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wav, sr = librosa.load(self.data_paths[idx], sr=16000)\n",
    "        mfcc = librosa.feature.mfcc(y=wav, sr=sr,n_mfcc=128, fmin=20.0, fmax=4000.0, hop_length=150,center=True)\n",
    "        # delta_mfcc  = librosa.feature.delta(mfccs)\n",
    "        # delta2_mfcc = librosa.feature.delta(mfccs, order=2)\n",
    "        # M = np.stack([mfccs, delta_mfcc, delta2_mfcc], axis=0)\n",
    "        stn = self.label[idx]\n",
    "        # print(mfcc.shape)\n",
    "        # padded_mfcc = torch.nn.functional.pad(mfcc, (0, self.max_seq_length - mfcc.shape[1]))\n",
    "        # mfcc = mfcc.T\n",
    "        if mfcc.shape[1] < self.max_seq_length:\n",
    "            padded_mfcc = np.pad(mfcc,((0,0),(0, self.max_seq_length - mfcc.shape[1])))\n",
    "        else:\n",
    "            padded_mfcc = mfcc[:,:1000]\n",
    "\n",
    "        tokens = ['<PAD>', '<SOS>', '<EOS>', 'ㄅ', 'ㄆ', 'ㄇ', 'ㄈ', 'ㄉ', 'ㄊ', 'ㄋ', 'ㄌ', 'ㄍ', 'ㄎ', 'ㄏ', 'ㄐ', 'ㄑ', 'ㄒ', 'ㄓ', 'ㄔ', 'ㄕ', 'ㄖ', 'ㄗ', 'ㄘ', 'ㄙ', 'ㄧ', 'ㄨ', 'ㄩ', 'ㄚ', 'ㄛ', 'ㄜ', 'ㄝ', 'ㄞ', 'ㄟ', 'ㄠ', 'ㄡ', 'ㄢ', 'ㄣ', 'ㄤ', 'ㄥ', 'ㄦ']  # List of tokens\n",
    "        token_to_idx = {token: idx for idx, token in enumerate(tokens)}\n",
    "        vocab_size = len(tokens)\n",
    "\n",
    "        train_targets = []\n",
    "\n",
    "        # Tokenize and convert sentences to token indices\n",
    "\n",
    "        tokenized = ['<SOS>'] + [token for token in stn] + ['<EOS>']\n",
    "        token_indices = [token_to_idx[token] for token in tokenized]\n",
    "        # train_targets.append(token_indices)\n",
    "        if len(token_indices) < self.stn_length:\n",
    "            token_indices = np.pad(token_indices,(0, self.stn_length - len(token_indices)))\n",
    "        \n",
    "        return padded_mfcc, token_indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_paths)\n",
    "    \n",
    "    def load_data(self):\n",
    "        word_dirs = []\n",
    "        for dirs in os_sorted(os.listdir(self.root_dir)):\n",
    "            paths = f'{self.root_dir}/{dirs}'\n",
    "            if os.path.isdir(paths):\n",
    "                word_dirs.append(paths)\n",
    "\n",
    "        for word_dir in word_dirs:\n",
    "                word = word_dir.split('/')[-1]\n",
    "                if word.find('clip') != -1:\n",
    "                    continue\n",
    "                for dirs in os.listdir(word_dir):\n",
    "                    data_path = f'{self.root_dir}/{word}/{dirs}'\n",
    "                    if data_path.endswith('.wav'):\n",
    "                        pinyin = lazy_pinyin(word, style=10)\n",
    "                        stn = []\n",
    "                        tmp = ''\n",
    "                        for char in pinyin:\n",
    "                            label = re.sub('[_˙ˊˇˋ]', '', char)   \n",
    "                            tmp += label\n",
    "                        self.data_paths.append(data_path)\n",
    "                        self.label.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Transformer architecture\n",
    "class TransformerSeq2Seq(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model, nhead, num_encoder_layers, num_decoder_layers):\n",
    "        super(TransformerSeq2Seq, self).__init__()\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead),\n",
    "            num_layers=num_encoder_layers\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead),\n",
    "            num_layers=num_decoder_layers,\n",
    "            \n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, output_dim)\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        encoder_output = self.encoder(src)\n",
    "        # print(trg.size())\n",
    "        # trg_mask = nn.Transformer.generate_square_subsequent_mask(self,trg.size(-1)).to(device)\n",
    "        # print(trg)\n",
    "        # print(trg_mask.size())\n",
    "        decoder_input = trg[:, :].unsqueeze(1)\n",
    "        decoder_input = decoder_input.type(torch.float)\n",
    "        \n",
    "        decoder_output = self.decoder(decoder_input, encoder_output)\n",
    "        output = self.fc_out(decoder_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 128\n",
    "output_dim = 37\n",
    "d_model = 1000\n",
    "nhead = 4\n",
    "num_encoder_layers = 4\n",
    "num_decoder_layers = 4\n",
    "learning_rate = 0.001\n",
    "batch_size = 20\n",
    "num_epochs = 10\n",
    "max_seq_length = 1000  # Maximum sequence length after padding\n",
    "stn_length = 1000\n",
    "root_dir = '/home/dmcl/yochen/VoiceData/users/msn9110/voice_data/sentence'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(root_dir, max_seq_length, stn_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokens and their mapping to indices\n",
    "tokens = ['<PAD>', '<SOS>', '<EOS>', 'ㄅ', 'ㄆ', 'ㄇ', 'ㄈ', 'ㄉ', 'ㄊ', 'ㄋ', 'ㄌ', 'ㄍ', 'ㄎ', 'ㄏ', 'ㄐ', 'ㄑ', 'ㄒ', 'ㄓ', 'ㄔ', 'ㄕ', 'ㄖ', 'ㄗ', 'ㄘ', 'ㄙ', 'ㄧ', 'ㄨ', 'ㄩ', 'ㄚ', 'ㄛ', 'ㄜ', 'ㄝ', 'ㄞ', 'ㄟ', 'ㄠ', 'ㄡ', 'ㄢ', 'ㄣ', 'ㄤ', 'ㄥ', 'ㄦ']  # List of tokens\n",
    "token_to_idx = {token: idx for idx, token in enumerate(tokens)}\n",
    "vocab_size = len(tokens)\n",
    "\n",
    "# Create model, loss function, and optimizer\n",
    "model = TransformerSeq2Seq(input_dim, vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=token_to_idx['<PAD>'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/366 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 1000])\n",
      "torch.Size([1, 800])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1) to match target batch_size (19980).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mprint\u001b[39m(output\u001b[39m.\u001b[39msize())\n\u001b[1;32m     14\u001b[0m target_tokens \u001b[39m=\u001b[39m target_tokens[:, \u001b[39m1\u001b[39m:]\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)  \u001b[39m# Shift target by one time step\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, target_tokens)\n\u001b[1;32m     16\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     17\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env2/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env2/lib/python3.9/site-packages/torch/nn/modules/loss.py:1120\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1120\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1121\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env2/lib/python3.9/site-packages/torch/nn/functional.py:2824\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2822\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2823\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 2824\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index)\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (1) to match target batch_size (19980)."
     ]
    }
   ],
   "source": [
    "# Training loop (same as before)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for mfcc, target_tokens in tqdm(train_loader):\n",
    "        mfcc = mfcc.to(device)\n",
    "        target_tokens = target_tokens.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(mfcc, target_tokens[:, :])  # Exclude the last token from target\n",
    "        print(target_tokens.size())\n",
    "        output = output.view(-1, vocab_size)\n",
    "        print(output.size())\n",
    "        target_tokens = target_tokens[:, 1:].reshape(-1)  # Shift target by one time step\n",
    "        loss = criterion(output, target_tokens)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {average_loss}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'transformer_seq2seq_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
